[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brendan Cooley",
    "section": "",
    "text": "GitHub    Twitter    LinkedIn    CV\nI currently work on Bayesian inference and computation and develop machine learning pipelines at Big League Advantage. I received my Ph.D. in Politics at Princeton University in 2020. In my dissertation research, I estimated a model of the world political economy to quantify how military coercion affects international trade. At Princeton, I taught the Politics Department’s summer math course for incoming graduate students, along with Daniel Gibbs.\nI’m interested in baseball, cities, open source software, and open science. I live in the Washington, DC area."
  },
  {
    "objectID": "index.html#some-stuff-i-worked-on-in-grad-school",
    "href": "index.html#some-stuff-i-worked-on-in-grad-school",
    "title": "Brendan Cooley",
    "section": "Some Stuff I Worked on in Grad School",
    "text": "Some Stuff I Worked on in Grad School\n\nDissertation\nTrade Policy in the Shadow of Power: Theory and Evidence on Economic Openness and Coercive Diplomacy\n   Introduction    Slides\n\n\nWorking Papers\nTrade Policy in the Shadow of Power: Quantifying Military Coercion in the International System\n   PDF   Slides\nGunshots and Turf Wars: Inferring Gang Territories from Shooting Reports (with Noam Reich)\n   PDF    Slides\nEstimating Policy Barriers to Trade\n   PDF    Slides\nGunboat Diplomacy: Political Bias, Trade Policy, and War\n   PDF    Slides    OPSC Talk\n\n\nTeaching\n\nIntroduction to Mathematics for Political Science (with Daniel Gibbs)"
  },
  {
    "objectID": "index.html#notes",
    "href": "index.html#notes",
    "title": "Brendan Cooley",
    "section": "Notes",
    "text": "Notes\nCalculating Historical Intercapital Distances\n  R code to calculate distances between historical capital cities, 1816-present\nRead and Clean ICEWS Coded Event Data\n  R code to read, clean, and count international dyadic event data"
  },
  {
    "objectID": "index.html#other",
    "href": "index.html#other",
    "title": "Brendan Cooley",
    "section": "Other",
    "text": "Other\nIn 2021 I recorded a podcast for Narratives with Will Jarvis. We chatted about why China and the United States don’t get along."
  },
  {
    "objectID": "content/notes/icews.html",
    "href": "content/notes/icews.html",
    "title": "Brendan Cooley",
    "section": "",
    "text": "Last updated 11 April 2018\nDatasets that researchers use to measure conflict in international relations are generally coarse. The Correlates of War (COW) Project’s Militarized Interstate Dispute data records threats, displays, and uses of military force between 1816 and 2010 and COW’s interstate war data records conflicts which resulted in at least 1,000 battle deaths. Both MIDs and wars are exceedingly unusual. Yet the “stuff” of international relations happens every day. Governments are bargaining and communicating all the time – sometimes cooperatively and sometimes conflictually. These interactions almost certaintly contain information about their proclivity to experience armed conflict. New data might help us measure and understand this “stuff” better.\nIn recent years, several very-large-n (&gt;1,000,000 observation) dyadic event datasets have become available for public use. An “event” takes the form of “[actor x] undertook [action z] toward [actor y] on [date w].” Natural language processors scrape newswires and map events into preexisting event and actor ontologies. The Integrated Crisis Early Warning System (ICEWS) is one such dataset. You can find a nice discussion of the project’s history by Phil Shrodt here. Andreas Beger and David Masad have nice writeups on what the data look like. It’s still pretty rare to see these data used in political science, however. See Gallop (2016), Minhas, Hoff, and Ward (2016), and Roberts and Tellez (2017) for notable exceptions.1\nThis may be because it’s still a little tricky to get these data into a format suitable for empirical analyses. Having struggled myself to clean ICEWS, I figured it’d be worth sharing my experience (working in R). I show three steps in the process here:\n\nGrabbing the data from dataverse\nConverting it ‘reduced’ form with conflict cooperation scores and COW codes, employing Phil Shrodt’s software\nConverting the ‘reduced’ data into date-dyad counts\n\nAs always, feel free to send along questions or comments or point out mistakes. That’s the point of open research. You can find all the software supporting this here.\nFirst, get the environment setup\npackages &lt;- c('dataverse', 'dplyr', 'zoo', 'lubridate', 'tidyr', 'bibtex', 'knitcitations')\nlapply(packages, require, character.only = TRUE)\n\nwrite.bib(packages)\nbib &lt;- read.bib('Rpackages.bib')\nThen, grab the data straight from Harvard’s dataverse\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\ndoi &lt;- \"doi:10.7910/DVN/28075\"\ndv &lt;- get_dataset(doi)\nWe can take a look at the files included in the dataverse repository\ndvFiles &lt;- dv$files$label\ndvFiles\n##  [1] \"BBN ACCENT Event Coding Evaluation.updated v01.pdf\"\n##  [2] \"CAMEO.CDB.09b5.pdf\"                                \n##  [3] \"changes.txt\"                                       \n##  [4] \"events.1995.20150313082510.tab.zip\"                \n##  [5] \"events.1996.20150313082528.tab.zip\"                \n##  [6] \"events.1997.20150313082554.tab.zip\"                \n##  [7] \"events.1998.20150313082622.tab.zip\"                \n##  [8] \"events.1999.20150313082705.tab.zip\"                \n##  [9] \"events.2000.20150313082808.tab.zip\"                \n## [10] \"events.2001.20150313082922.tab.zip\"                \n## [11] \"events.2002.20150313083053.tab.zip\"                \n## [12] \"events.2003.20150313083228.tab.zip\"                \n## [13] \"events.2004.20150313083407.tab.zip\"                \n## [14] \"events.2005.20150313083555.tab.zip\"                \n## [15] \"events.2006.20150313083752.tab.zip\"                \n## [16] \"events.2007.20150313083959.tab.zip\"                \n## [17] \"events.2008.20150313084156.tab.zip\"                \n## [18] \"events.2009.20150313084349.tab.zip\"                \n## [19] \"events.2010.20150313084533.tab.zip\"                \n## [20] \"events.2011.20150313084656.tab.zip\"                \n## [21] \"events.2012.20150313084811.tab.zip\"                \n## [22] \"events.2013.20150313084929.tab.zip\"                \n## [23] \"events.2014.20160121105408.tab\"                    \n## [24] \"events.2015.20170206133646.tab\"                    \n## [25] \"events.2016.20180122103653.tab\"                    \n## [26] \"events.2017.20180122111453.tab\"                    \n## [27] \"ICEWS Coded Event Data Read Me.pdf\"                \n## [28] \"ICEWS Events and Aggregations.pdf\"                 \n## [29] \"ICEWS Expanded CAMEO Annotation Guidelines.pdf\"\nWe want to get the files prefixed with ‘events.’ We’ll unzip them and put them in a folder called /rawICEWS.\ndir.create('rawICEWS', showWarnings = FALSE)\n\n# search through all files in dataverse repo\nfor (i in dvFiles) {\n  # for those that start with events\n  if (substr(i, 1, 6) == 'events') {\n    dest &lt;- paste0('rawICEWS/', i)\n    writeBin(get_file(i, doi), dest)\n    # unzip those that are compressed\n    if (substr(i, nchar(i) - 2, nchar(i)) == 'zip') {\n      unzip(dest, exdir='rawICEWS/')\n      file.remove(dest)  # trash zipfile\n    }\n  }\n}\n\n# store list of files in .txt\nfNames &lt;- paste0('rawICEWS/', list.files('rawICEWS'))\nlapply(fNames, write, 'fNames.txt', append=TRUE)\nThis will take a minute or two. Go get some coffee. Alternatively, you can download the raw data (as of April 2018) from my Dropbox and plop it into your /rawICEWS folder in your working directory. The nice thing about this code is that it should dynamically grab new data as the ICEWS project uploads it to dataverse.\nEither way, now we have all of our raw data ready to go sitting in /rawICEWS. The raw ICEWS data is clunky on several dimensions. Phil Shrodt provides software to get it into a format that looks recognizable to empirical international relations researchers. If you’re interested in the machinery, check it out here.\nFor our purposes, we just need the script text_to_CAMEO.py and the ontology files agentnames.txt and countrynames.txt. It’ll take the list of filenames and convert them into “reduced” form. Just navigate to the current working directory and run\npython text_to_CAMEO.py -c -t fNames.txt\nAfter this you should have a bunch of .txt files sitting in your working directory. I moved them over to a /reducedICEWS folder and you can find them here. Now we can load these into memory and get to work.\n# helper to replace empty cells with NAs\nempty_as_na &lt;- function(x) {\n  ifelse(as.character(x)!=\"\", x, NA)\n}\n\nreducedFiles &lt;- list.files(\"reducedICEWS\")\nevents.Y &lt;- list()  # list holding data frames for each year\n# for each of the reduced files\nfor (i in 1:length(reducedFiles)) {\n  # append to list\n  events.Y[[i]] &lt;- read.delim(paste('reducedICEWS/', reducedFiles[i], sep=\"\"), header = F)\n  # convert column names\n  colnames(events.Y[[i]]) &lt;- c(\"date\", \"sourceName\", \"sourceCOW\", \"sourceSec\",\n                               \"tarName\", \"tarCOW\", \"tarSec\", \"CAMEO\", \"Goldstein\", \"quad\")\n  # replace empty cells with NAs\n  events.Y[[i]] %&gt;% mutate_if(is.factor, as.character) %&gt;% mutate_all(funs(empty_as_na)) %&gt;% as_tibble() -&gt; events.Y[[i]]\n}\n# bind everything together\nevents &lt;- bind_rows(events.Y)\n\n# add year and month fields\nevents$month &lt;- as.yearmon(events$date)\nevents$year &lt;- year(events$date)\nevents &lt;- events %&gt;% select(date, year, month, everything())\nThis will take a little while too (this is BIG data, people!). But once it’s done, we can take a look at what we’ve got:\nhead(events)\n## # A tibble: 6 x 12\n##   date    year month  sourceName sourceCOW sourceSec tarName tarCOW tarSec\n##   &lt;chr&gt;  &lt;dbl&gt; &lt;S3: &gt; &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt; &lt;chr&gt; \n## 1 1995-… 1995. Jan 1… RUS              365 REB       RUS        365 GOV   \n## 2 1995-… 1995. Jan 1… BIH              346 GOV       SRB        345 CVL   \n## 3 1995-… 1995. Jan 1… SRB              345 CVL       BIH        346 GOV   \n## 4 1995-… 1995. Jan 1… CAN               20 OTH       CAN         20 GOV   \n## 5 1995-… 1995. Jan 1… CAN               20 JUD       CAN         20 GOV   \n## 6 1995-… 1995. Jan 1… RUS              365 GOV       RUS        365 OTH   \n## # ... with 3 more variables: CAMEO &lt;int&gt;, Goldstein &lt;dbl&gt;, quad &lt;int&gt;\nNow we have COW-coded countries (sourceCOW, tarCOW), the date actors within these countries interacted, and the sector identity of the actors (the data includes government-government interactions but also interactions between subnational actors). The CAMEO/Goldstein/quad fields describe the nature of the interaction under different event categorization systems. See the Shrodt documentation for more detail on the event and sector classifications. The quad score (ranging from 1-4) classifies events as either conflictual or cooperative and as verbal or material in nature. A score of 1 corresponds to verbal cooperation. So the first event can be read as follows: “On Jan. 1, 1995, rebels in Russia verbally cooperated with the government in Russia.” The CAMEO scores provide a much richer event classification.\nThe first question we almost always want to answer – how many times did each pair of countries experience each type of interaction with each other? The following function returns the directed dyadic matrix of counts, aggregated by ‘year’ or ‘month’.\nevent.counts &lt;- function(events, agg.date=c('month', 'year'), code=c('quad', 'CAMEO')) {\n  counts &lt;- events %&gt;%\n    group_by_(agg.date, 'sourceCOW', 'tarCOW', code) %&gt;%\n    summarise(n = n()) %&gt;%\n    ungroup()  # this seems trivial but screws up a lot of stuff if you don't do it\n  output &lt;- spread_(counts, code, 'n')\n  output[is.na(output)] &lt;- 0\n  return(output)\n}\n\ncounts &lt;- event.counts(events, 'year', 'quad')\nhead(counts)\n## # A tibble: 6 x 7\n##    year sourceCOW tarCOW   `1`   `2`   `3`   `4`\n##   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 1995.        0.     0. 1440.   60.  357.  259.\n## 2 1995.        0.     2.  587.   19.  123.   14.\n## 3 1995.        0.    20.   86.    1.   10.    1.\n## 4 1995.        0.    31.    1.    0.    0.    0.\n## 5 1995.        0.    40.   69.    2.    1.    2.\n## 6 1995.        0.    41.   25.    8.    3.    3.\nCOW code 0 is international organizations (IOs), so we can get a sense of how many times IOs are interacting with themselves and other countries in the data from the top of this table. And now that we have the count data we can start doing fun stuff. Over here I’ve built an app that allows users to scale subsets of the data, following Lowe.2 The procedures implemented there allow us to recover what can be thought of as a conflict-cooperation “score” for each dyad-year, based on the underlying count data.\n\n\nLeeper TJ (2017). dataverse: R Client for Dataverse 4. R package version 0.2.0.\nWickham H, Francois R, Henry L and Müller K (2017). dplyr: A Grammar of Data Manipulation. R package version 0.7.4, &lt;URL: https://CRAN.R-project.org/package=dplyr&gt;.\nZeileis A and Grothendieck G (2005). “zoo: S3 Infrastructure for Regular and Irregular Time Series.” Journal of Statistical Software, 14(6), pp. 1-27. doi: 10.18637/jss.v014.i06 (URL: http://doi.org/10.18637/jss.v014.i06).\nGrolemund G and Wickham H (2011). “Dates and Times Made Easy with lubridate.” Journal of Statistical Software, 40(3), pp. 1-25. &lt;URL: http://www.jstatsoft.org/v40/i03/&gt;.\nWickham H and Henry L (2018). tidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions. R package version 0.8.0, &lt;URL: https://CRAN.R-project.org/package=tidyr&gt;.\nFrancois R (2017). bibtex: Bibtex Parser. R package version 0.4.2, &lt;URL: https://CRAN.R-project.org/package=bibtex&gt;.\nBoettiger C (2017). knitcitations: Citations for ‘Knitr’ Markdown Files. R package version 1.0.8, &lt;URL: https://CRAN.R-project.org/package=knitcitations&gt;.\n\n\n\nGallop, Max B. 2016. “Endogenous networks and international cooperation.” Journal of Peace Research 53 (3):310–24.\nMinhas, Shahryar, Peter D Hoff, and Michael D Ward. 2016. “A new approach to analyzing coevolving longitudinal networks in international relations.” Journal of Peace Research 53 (3):491–505.\nRoberts, Jordan, and Juan Tellez. 2017. “Freedom House ’s Scarlet Letter: Assessment Power through Transnational Pressure.”"
  },
  {
    "objectID": "content/notes/icews.html#read-and-clean-icews-coded-event-data",
    "href": "content/notes/icews.html#read-and-clean-icews-coded-event-data",
    "title": "Brendan Cooley",
    "section": "",
    "text": "Last updated 11 April 2018\nDatasets that researchers use to measure conflict in international relations are generally coarse. The Correlates of War (COW) Project’s Militarized Interstate Dispute data records threats, displays, and uses of military force between 1816 and 2010 and COW’s interstate war data records conflicts which resulted in at least 1,000 battle deaths. Both MIDs and wars are exceedingly unusual. Yet the “stuff” of international relations happens every day. Governments are bargaining and communicating all the time – sometimes cooperatively and sometimes conflictually. These interactions almost certaintly contain information about their proclivity to experience armed conflict. New data might help us measure and understand this “stuff” better.\nIn recent years, several very-large-n (&gt;1,000,000 observation) dyadic event datasets have become available for public use. An “event” takes the form of “[actor x] undertook [action z] toward [actor y] on [date w].” Natural language processors scrape newswires and map events into preexisting event and actor ontologies. The Integrated Crisis Early Warning System (ICEWS) is one such dataset. You can find a nice discussion of the project’s history by Phil Shrodt here. Andreas Beger and David Masad have nice writeups on what the data look like. It’s still pretty rare to see these data used in political science, however. See Gallop (2016), Minhas, Hoff, and Ward (2016), and Roberts and Tellez (2017) for notable exceptions.1\nThis may be because it’s still a little tricky to get these data into a format suitable for empirical analyses. Having struggled myself to clean ICEWS, I figured it’d be worth sharing my experience (working in R). I show three steps in the process here:\n\nGrabbing the data from dataverse\nConverting it ‘reduced’ form with conflict cooperation scores and COW codes, employing Phil Shrodt’s software\nConverting the ‘reduced’ data into date-dyad counts\n\nAs always, feel free to send along questions or comments or point out mistakes. That’s the point of open research. You can find all the software supporting this here.\nFirst, get the environment setup\npackages &lt;- c('dataverse', 'dplyr', 'zoo', 'lubridate', 'tidyr', 'bibtex', 'knitcitations')\nlapply(packages, require, character.only = TRUE)\n\nwrite.bib(packages)\nbib &lt;- read.bib('Rpackages.bib')\nThen, grab the data straight from Harvard’s dataverse\nSys.setenv(\"DATAVERSE_SERVER\" = \"dataverse.harvard.edu\")\ndoi &lt;- \"doi:10.7910/DVN/28075\"\ndv &lt;- get_dataset(doi)\nWe can take a look at the files included in the dataverse repository\ndvFiles &lt;- dv$files$label\ndvFiles\n##  [1] \"BBN ACCENT Event Coding Evaluation.updated v01.pdf\"\n##  [2] \"CAMEO.CDB.09b5.pdf\"                                \n##  [3] \"changes.txt\"                                       \n##  [4] \"events.1995.20150313082510.tab.zip\"                \n##  [5] \"events.1996.20150313082528.tab.zip\"                \n##  [6] \"events.1997.20150313082554.tab.zip\"                \n##  [7] \"events.1998.20150313082622.tab.zip\"                \n##  [8] \"events.1999.20150313082705.tab.zip\"                \n##  [9] \"events.2000.20150313082808.tab.zip\"                \n## [10] \"events.2001.20150313082922.tab.zip\"                \n## [11] \"events.2002.20150313083053.tab.zip\"                \n## [12] \"events.2003.20150313083228.tab.zip\"                \n## [13] \"events.2004.20150313083407.tab.zip\"                \n## [14] \"events.2005.20150313083555.tab.zip\"                \n## [15] \"events.2006.20150313083752.tab.zip\"                \n## [16] \"events.2007.20150313083959.tab.zip\"                \n## [17] \"events.2008.20150313084156.tab.zip\"                \n## [18] \"events.2009.20150313084349.tab.zip\"                \n## [19] \"events.2010.20150313084533.tab.zip\"                \n## [20] \"events.2011.20150313084656.tab.zip\"                \n## [21] \"events.2012.20150313084811.tab.zip\"                \n## [22] \"events.2013.20150313084929.tab.zip\"                \n## [23] \"events.2014.20160121105408.tab\"                    \n## [24] \"events.2015.20170206133646.tab\"                    \n## [25] \"events.2016.20180122103653.tab\"                    \n## [26] \"events.2017.20180122111453.tab\"                    \n## [27] \"ICEWS Coded Event Data Read Me.pdf\"                \n## [28] \"ICEWS Events and Aggregations.pdf\"                 \n## [29] \"ICEWS Expanded CAMEO Annotation Guidelines.pdf\"\nWe want to get the files prefixed with ‘events.’ We’ll unzip them and put them in a folder called /rawICEWS.\ndir.create('rawICEWS', showWarnings = FALSE)\n\n# search through all files in dataverse repo\nfor (i in dvFiles) {\n  # for those that start with events\n  if (substr(i, 1, 6) == 'events') {\n    dest &lt;- paste0('rawICEWS/', i)\n    writeBin(get_file(i, doi), dest)\n    # unzip those that are compressed\n    if (substr(i, nchar(i) - 2, nchar(i)) == 'zip') {\n      unzip(dest, exdir='rawICEWS/')\n      file.remove(dest)  # trash zipfile\n    }\n  }\n}\n\n# store list of files in .txt\nfNames &lt;- paste0('rawICEWS/', list.files('rawICEWS'))\nlapply(fNames, write, 'fNames.txt', append=TRUE)\nThis will take a minute or two. Go get some coffee. Alternatively, you can download the raw data (as of April 2018) from my Dropbox and plop it into your /rawICEWS folder in your working directory. The nice thing about this code is that it should dynamically grab new data as the ICEWS project uploads it to dataverse.\nEither way, now we have all of our raw data ready to go sitting in /rawICEWS. The raw ICEWS data is clunky on several dimensions. Phil Shrodt provides software to get it into a format that looks recognizable to empirical international relations researchers. If you’re interested in the machinery, check it out here.\nFor our purposes, we just need the script text_to_CAMEO.py and the ontology files agentnames.txt and countrynames.txt. It’ll take the list of filenames and convert them into “reduced” form. Just navigate to the current working directory and run\npython text_to_CAMEO.py -c -t fNames.txt\nAfter this you should have a bunch of .txt files sitting in your working directory. I moved them over to a /reducedICEWS folder and you can find them here. Now we can load these into memory and get to work.\n# helper to replace empty cells with NAs\nempty_as_na &lt;- function(x) {\n  ifelse(as.character(x)!=\"\", x, NA)\n}\n\nreducedFiles &lt;- list.files(\"reducedICEWS\")\nevents.Y &lt;- list()  # list holding data frames for each year\n# for each of the reduced files\nfor (i in 1:length(reducedFiles)) {\n  # append to list\n  events.Y[[i]] &lt;- read.delim(paste('reducedICEWS/', reducedFiles[i], sep=\"\"), header = F)\n  # convert column names\n  colnames(events.Y[[i]]) &lt;- c(\"date\", \"sourceName\", \"sourceCOW\", \"sourceSec\",\n                               \"tarName\", \"tarCOW\", \"tarSec\", \"CAMEO\", \"Goldstein\", \"quad\")\n  # replace empty cells with NAs\n  events.Y[[i]] %&gt;% mutate_if(is.factor, as.character) %&gt;% mutate_all(funs(empty_as_na)) %&gt;% as_tibble() -&gt; events.Y[[i]]\n}\n# bind everything together\nevents &lt;- bind_rows(events.Y)\n\n# add year and month fields\nevents$month &lt;- as.yearmon(events$date)\nevents$year &lt;- year(events$date)\nevents &lt;- events %&gt;% select(date, year, month, everything())\nThis will take a little while too (this is BIG data, people!). But once it’s done, we can take a look at what we’ve got:\nhead(events)\n## # A tibble: 6 x 12\n##   date    year month  sourceName sourceCOW sourceSec tarName tarCOW tarSec\n##   &lt;chr&gt;  &lt;dbl&gt; &lt;S3: &gt; &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt; &lt;chr&gt; \n## 1 1995-… 1995. Jan 1… RUS              365 REB       RUS        365 GOV   \n## 2 1995-… 1995. Jan 1… BIH              346 GOV       SRB        345 CVL   \n## 3 1995-… 1995. Jan 1… SRB              345 CVL       BIH        346 GOV   \n## 4 1995-… 1995. Jan 1… CAN               20 OTH       CAN         20 GOV   \n## 5 1995-… 1995. Jan 1… CAN               20 JUD       CAN         20 GOV   \n## 6 1995-… 1995. Jan 1… RUS              365 GOV       RUS        365 OTH   \n## # ... with 3 more variables: CAMEO &lt;int&gt;, Goldstein &lt;dbl&gt;, quad &lt;int&gt;\nNow we have COW-coded countries (sourceCOW, tarCOW), the date actors within these countries interacted, and the sector identity of the actors (the data includes government-government interactions but also interactions between subnational actors). The CAMEO/Goldstein/quad fields describe the nature of the interaction under different event categorization systems. See the Shrodt documentation for more detail on the event and sector classifications. The quad score (ranging from 1-4) classifies events as either conflictual or cooperative and as verbal or material in nature. A score of 1 corresponds to verbal cooperation. So the first event can be read as follows: “On Jan. 1, 1995, rebels in Russia verbally cooperated with the government in Russia.” The CAMEO scores provide a much richer event classification.\nThe first question we almost always want to answer – how many times did each pair of countries experience each type of interaction with each other? The following function returns the directed dyadic matrix of counts, aggregated by ‘year’ or ‘month’.\nevent.counts &lt;- function(events, agg.date=c('month', 'year'), code=c('quad', 'CAMEO')) {\n  counts &lt;- events %&gt;%\n    group_by_(agg.date, 'sourceCOW', 'tarCOW', code) %&gt;%\n    summarise(n = n()) %&gt;%\n    ungroup()  # this seems trivial but screws up a lot of stuff if you don't do it\n  output &lt;- spread_(counts, code, 'n')\n  output[is.na(output)] &lt;- 0\n  return(output)\n}\n\ncounts &lt;- event.counts(events, 'year', 'quad')\nhead(counts)\n## # A tibble: 6 x 7\n##    year sourceCOW tarCOW   `1`   `2`   `3`   `4`\n##   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 1995.        0.     0. 1440.   60.  357.  259.\n## 2 1995.        0.     2.  587.   19.  123.   14.\n## 3 1995.        0.    20.   86.    1.   10.    1.\n## 4 1995.        0.    31.    1.    0.    0.    0.\n## 5 1995.        0.    40.   69.    2.    1.    2.\n## 6 1995.        0.    41.   25.    8.    3.    3.\nCOW code 0 is international organizations (IOs), so we can get a sense of how many times IOs are interacting with themselves and other countries in the data from the top of this table. And now that we have the count data we can start doing fun stuff. Over here I’ve built an app that allows users to scale subsets of the data, following Lowe.2 The procedures implemented there allow us to recover what can be thought of as a conflict-cooperation “score” for each dyad-year, based on the underlying count data.\n\n\nLeeper TJ (2017). dataverse: R Client for Dataverse 4. R package version 0.2.0.\nWickham H, Francois R, Henry L and Müller K (2017). dplyr: A Grammar of Data Manipulation. R package version 0.7.4, &lt;URL: https://CRAN.R-project.org/package=dplyr&gt;.\nZeileis A and Grothendieck G (2005). “zoo: S3 Infrastructure for Regular and Irregular Time Series.” Journal of Statistical Software, 14(6), pp. 1-27. doi: 10.18637/jss.v014.i06 (URL: http://doi.org/10.18637/jss.v014.i06).\nGrolemund G and Wickham H (2011). “Dates and Times Made Easy with lubridate.” Journal of Statistical Software, 40(3), pp. 1-25. &lt;URL: http://www.jstatsoft.org/v40/i03/&gt;.\nWickham H and Henry L (2018). tidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions. R package version 0.8.0, &lt;URL: https://CRAN.R-project.org/package=tidyr&gt;.\nFrancois R (2017). bibtex: Bibtex Parser. R package version 0.4.2, &lt;URL: https://CRAN.R-project.org/package=bibtex&gt;.\nBoettiger C (2017). knitcitations: Citations for ‘Knitr’ Markdown Files. R package version 1.0.8, &lt;URL: https://CRAN.R-project.org/package=knitcitations&gt;.\n\n\n\nGallop, Max B. 2016. “Endogenous networks and international cooperation.” Journal of Peace Research 53 (3):310–24.\nMinhas, Shahryar, Peter D Hoff, and Michael D Ward. 2016. “A new approach to analyzing coevolving longitudinal networks in international relations.” Journal of Peace Research 53 (3):491–505.\nRoberts, Jordan, and Juan Tellez. 2017. “Freedom House ’s Scarlet Letter: Assessment Power through Transnational Pressure.”"
  },
  {
    "objectID": "content/notes/icews.html#footnotes",
    "href": "content/notes/icews.html#footnotes",
    "title": "Brendan Cooley",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlease feel free point out others I’m missing!↩︎\nThis is still in-progress, I’ve been meaning to go back and update it.]↩︎"
  },
  {
    "objectID": "content/notes/dists.html",
    "href": "content/notes/dists.html",
    "title": "Brendan Cooley",
    "section": "",
    "text": "Last updated 10 January 2018\nMilitary power degrades with distance. Fighting a war on another continent is, for most militaries, more difficult than fighting at home. Studies of conflict frequently employ capital-to-capital distance (or some transformation of this metric) as one proxy for this loss of strength from power projection (see, for example, (Gartzke and Braithwaite (2011)). Combined with data on territorial contiguity, these metrics can provide us with a picture of the geographic constraints facing countries contemplating war with one another.\nHistorical intercapital distance data proved difficult to find however. Most researchers appear to rely on EuGene to generate this data. I wasn’t able to track down any documentation on exactly how EuGene does this, however.1 Gleditsch and Ward (2001) generated a minimum interstate distance dataset, but their data only covers the post-1875 period. For researchers using Correlates of War data, distance data would ideally cover 1816 to the present.\nIt turns out that it’s not too difficult to build intercapital distance data from scratch, however. All that is required is data on the names of capital cities for each state system member for every year between 1816 and the present. Paul Hensel’s ICOW Historical State Names dataset provides this information. We can then use Google’s Geocoding API through the ggmap R package to get the coordinates of each historical capital, which can then be used to generate intercapital distance matrices.\nHere, I show how to conduct this exercise in R. A clean version of Hensel’s historical capital dataset is available here. I’ve included all of the data and software necessary to generate this data on Github. Feel free to send along questions, comments, or suggestions for improvement to bcooley@princeton.edu.\n\n\nStart by loading up the packages we’ll need for analysis:\nlibs &lt;- c('readr', 'dplyr', 'tidyr', 'ggmap', 'geosphere', 'leaflet', 'knitr', 'bibtex', 'knitcitations')\nsapply(libs, require, character.only = TRUE)\nI started by cleaning up Hensel’s data a bit in excel. Each capital city is listed as its own observation, along with the country, its COW code, and the first and last year the city served as a capital. Because the Hensel data is coded annually, I take the country’s capital at the start of any given year to be its capital for that entire year. For countries that no longer exist (e.g. Mecklenburg-Schwerin) I provide a contemporary alternative country name (aName) to help Google locate the city’s coordinates. The assumption underlying this procedure is that the cities that served as capitals historically have not moved from their historical location (if I’m missing any instances where this occured please let me know). We can take a look at the data below:\n# load data\ndists &lt;- read_csv('capitals.csv')\n\n# Hensel's data run from 1800-2016, set bounds\ndists$startDate &lt;- ifelse(is.na(dists$startDate), 1800, dists$startDate)\ndists$endDate &lt;- ifelse(is.na(dists$endDate), 2016, dists$endDate)\n\nhead(dists)\n## # A tibble: 6 x 6\n##   ccode Name                     aName startDate endDate Capital\n##   &lt;int&gt; &lt;chr&gt;                    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n## 1     2 United States of America &lt;NA&gt;       1800    2016 Washington D.C.\n## 2    20 Canada                   &lt;NA&gt;       1800    1841 Ottawa\n## 3    20 Canada                   &lt;NA&gt;       1841    1843 Kingston\n## 4    20 Canada                   &lt;NA&gt;       1843    1849 Montreal\n## 5    20 Canada                   &lt;NA&gt;       1849    1859 Toronto\n## 6    20 Canada                   &lt;NA&gt;       1859    1865 Quebec City\nNow we need to get the cities’ coordinates. I simply feed the City, Country tuples to ggmap’s geocode function, which returns lat, lng coordinates if it can find a match in Google’s database. If the country has a contemporary name, I use this name for the search in lieu of its old name. I save these coordinates to the dists data frame when they are found. The geocode API imposes some query limits, so it sometimes throws an OVER_QUERY_LIMIT error. If cities remain uncoded, I simply run the loop again, skipping over those that already have coordinates. We can check that the coding worked with the dists %&gt;% filter(is.na(lat)), which should return an empty data frame if all cities have been geocoded.\n# initialize coordinates\ndists$lat &lt;- NA\ndists$lng &lt;- NA\n\n# run until all cities have been coded\nwhile(nrow(dists %&gt;% filter(is.na(lat))) &gt; 0) {\n  # for each city\n  for (i in 1:nrow(dists)) {\n    # skip if it's already been coded\n    if (is.na(dists[i,]$lat)) {\n      if (is.na(dists[i,]$aName)) {\n        # query City, Country for each capital\n        query &lt;- paste0(dists[i, ]$Capital, \", \", dists[i, ]$Name)\n        latlng &lt;- geocode(query)\n        dists[i, ]$lat &lt;- latlng$lat\n        dists[i, ]$lng &lt;- latlng$lon\n      }\n      else {\n        # use alternative country name\n        query &lt;- paste0(dists[i, ]$Capital, \", \", dists[i, ]$aName)\n        latlng &lt;- geocode(query)\n        dists[i, ]$lat &lt;- latlng$lat\n        dists[i, ]$lng &lt;- latlng$lon\n      }\n    }\n  }\n}\nWe can check to make sure the cities were coded correctly by plotting them on a map. Clicking on the city will show a popup with its name. I poked around this map a bit and everything seemed to land in the right spot.\nleaflet(data=dists) %&gt;% addProviderTiles(providers$CartoDB.Positron, options=providerTileOptions(minZoom = 1)) %&gt;%\n  addCircleMarkers(~lng, ~lat, popup=~Capital, radius=1) %&gt;%\n  setMaxBounds(-180, -90, 180, 90)\n\n\nI then export the geocoded capitals as a csv. You can find these data here.\nwrite_csv(dists, 'dists.csv')\n\n\n\nRemember that the point of all this was to generate intercapital distance data for all countries in the COW data. To do this, we want to convert our geocoded capital city data into a dyadic time series that gives the distance between any two countries’ capitals for a given year. If there are \\(N\\) countries in the system in a given year \\(t\\), we want to be able to generate an \\(N \\times N\\) matrix for that year where each entry is the distance between countries \\(i\\) and \\(j\\).\nI start by loading up COW’s state system membership data, so we know which countries were members of the system for each year. The field “styear” denotes the year the country entered the system, and the field “endyear” gives the year it exited. I then append this data to the distance data. The output is shown below.\ndists &lt;- read_csv('dists.csv')\n\n# get state system membership (COW)\nsysMemUrl &lt;- \"http://www.correlatesofwar.org/data-sets/state-system-membership/states2016/at_download/file\"\n\nsysMem &lt;- read_csv(sysMemUrl) %&gt;% select(ccode, styear, endyear)\ndists &lt;- left_join(dists, sysMem, by=\"ccode\")\n\nhead(dists)\n## # A tibble: 6 x 10\n##   ccode Name         aName startD… endDa… Capital    lat   lng stye… endy…\n##   &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt;  &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n## 1     2 United Stat… &lt;NA&gt;     1800   2016 Washing…  38.9 -77.0  1816  2016\n## 2    20 Canada       &lt;NA&gt;     1800   1841 Ottawa    45.4 -75.7  1920  2016\n## 3    20 Canada       &lt;NA&gt;     1841   1843 Kingston  44.2 -76.5  1920  2016\n## 4    20 Canada       &lt;NA&gt;     1843   1849 Montreal  45.5 -73.6  1920  2016\n## 5    20 Canada       &lt;NA&gt;     1849   1859 Toronto   43.7 -79.4  1920  2016\n## 6    20 Canada       &lt;NA&gt;     1859   1865 Quebec …  46.8 -71.2  1920  2016\nFrom this dataframe, I can build the \\(N \\times N\\) intercapital distance matrix for any year. I wrap this in a function coord2DistM, which takes the desired year and the dists dataframe as arguments. It filters the distance data to include only states that were active in that year. It then feeds the latitude and longitude coordinates to the distm function from the geosphere package, which retuns the desired distance matrix. I convert all distances to kilometers.\nThe function distM2dydist takes this distance matrix and a pair of COW country codes and returns the dyadic distance. Below, I show how to use these functions to build the intercapital distance matrix for the year 1816 and get the distance between Britain and Saxony.\n# Note: assigns capital to city that was capital at beginning of year\n\ncoord2DistM &lt;- function(dists, year) {\n  # filter by system membership, then relevant capital\n  distsY &lt;- dists %&gt;% filter(styear &lt;= year, endyear &gt;= year) %&gt;% filter(startDate &lt; year & endDate &gt;= year)\n  # check that one capital returned per country\n  if (length(unique(distsY$Capital)) != length(unique(distsY$Name))) {\n    print('error: nCountries != nCapitals, check underlying coordinate data')\n  }\n  else {\n    # get distance matrix for selected year\n    latlng &lt;- distsY %&gt;% select('lng', 'lat') %&gt;% as.matrix()\n    distsYmatrix &lt;- distm(latlng, latlng, fun=distVincentySphere)\n    distsYmatrix &lt;- distsYmatrix / 1000  # convert to km\n    rownames(distsYmatrix) &lt;- colnames(distsYmatrix) &lt;- distsY$ccode\n    return(distsYmatrix)\n  }\n}\n\n# get distance between i and j\ndistM2dydist &lt;- function(distM, ccode1, ccode2) {\n  return(distM[ccode1, ccode2])\n}\n\n# application\nyear &lt;- 1816\nBritain &lt;- \"200\"\nSaxony &lt;- \"269\"\n\ndistM1816 &lt;- coord2DistM(dists, year)\ndistM2dydist(distM1816, Britain, Saxony)\n## [1] 965.3628\nThese functions can be used in tandem to grab intercapital distances for arbitrary year, dyad pairings.\n\n\n\nConflict researchers often want this data to analyze wars. Now I show how to merge intercapital distance data with COW’s inter-state-war data. I clean up the COW war data a bit, which you can see below post-cleaning. In the most basic leve, the data give information about the belligerents in every war and how long each war lasted.\n# append to COW wars data\nwarUrl &lt;- \"http://www.correlatesofwar.org/data-sets/COW-war/inter-state-war-data/at_download/file\"\ncowWars &lt;- read_csv(warUrl)\n\n# for simplicity, ignore armistices\ncowWars$StartYear &lt;- cowWars$StartYear1\ncowWars$EndYear &lt;- ifelse(cowWars$EndYear2 == -8, cowWars$EndYear1, cowWars$EndYear2)\ncowWars &lt;- cowWars %&gt;% select(WarName, ccode, StateName, Side, StartYear, EndYear)\n\nhead(cowWars)\n## # A tibble: 6 x 6\n##   WarName             ccode StateName                 Side StartYear EndY…\n##   &lt;chr&gt;               &lt;int&gt; &lt;chr&gt;                    &lt;int&gt;     &lt;int&gt; &lt;int&gt;\n## 1 Franco-Spanish War    230 Spain                        2      1823  1823\n## 2 Franco-Spanish War    220 France                       1      1823  1823\n## 3 First Russo-Turkish   640 Ottoman Empire               2      1828  1829\n## 4 First Russo-Turkish   365 Russia                       1      1828  1829\n## 5 Mexican-American       70 Mexico                       2      1846  1847\n## 6 Mexican-American        2 United States of America     1      1846  1847\nWe want to know the intercapital distance between each pair of belligerents between 1816 and the present. We could use the coord2DistM and distM2dydist but this would require calculating the intercapital distance matrix for every year in which there was a war. A simpler solution is to build a dataframe of capital-years, along with their coordinates, merge this with the war data, and calculate the distance between belligerents, given their capitals’ coordinates.\nI used the procedure in this stackoverflow post to create the capital-year dataframe. Once merged, I use geosphere’s distVicentySphere function to calculate the distances. The resulting data is shown below the code.\n# convert capital data to yearly observations\n# https://stackoverflow.com/questions/28553762/expand-year-range-in-r\ndistsYear &lt;- dists\ndistsYear$year &lt;- mapply(seq, distsYear$startDate, distsYear$endDate, SIMPLIFY=FALSE)\ndistsYear &lt;- distsYear %&gt;%\n  unnest(year) %&gt;%\n  select(ccode, year, lat, lng)\n\n# get capital in start year\ncowWars$year &lt;- cowWars$StartYear\n\n# append coords for each side\ncowWars1 &lt;- cowWars %&gt;% filter(Side == 1) %&gt;% left_join(distsYear, by=c(\"ccode\", \"year\")) %&gt;% rename(State1 = StateName, ccode1 = ccode, lat1 = lat, lng1 = lng) %&gt;% select(-Side)\ncowWars2 &lt;- cowWars %&gt;% filter(Side == 2) %&gt;% left_join(distsYear, by=c(\"ccode\", \"year\")) %&gt;% rename(State2 = StateName, ccode2 = ccode, lat2 = lat, lng2 = lng) %&gt;% select(WarName, State2, ccode2, lat2, lng2)\n\ncowWarsDyadic &lt;- left_join(cowWars1, cowWars2, by=\"WarName\") %&gt;% select(WarName, ccode1, State1, ccode2, State2, year, lng1, lat1, lng2, lat2)\n\n# calculate distance\nlatlng1 &lt;- cowWarsDyadic %&gt;% select(lng1, lat1)\nlatlng2 &lt;- cowWarsDyadic %&gt;% select(lng2, lat2)\ncowWarsDyadic$distance &lt;- distVincentySphere(latlng1, latlng2) / 1000  # convert to km\n\n# export data\nwrite_csv(cowWarsDyadic, 'cowWarsDist.csv')\n\ncowWarsDyadic %&gt;% select(WarName, ccode1, State1, ccode2, State2, distance)\n## # A tibble: 813 x 6\n##    WarName                  ccode1 State1          ccode2 State2    dista…\n##    &lt;chr&gt;                     &lt;int&gt; &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n##  1 Franco-Spanish War          220 France             230 Spain       1054\n##  2 Franco-Spanish War          220 France             230 Spain       1054\n##  3 First Russo-Turkish         365 Russia             640 Ottoman …   2233\n##  4 Mexican-American              2 United States …     70 Mexico      3035\n##  5 Austro-Sardinian            300 Austria            337 Tuscany      632\n##  6 Austro-Sardinian            300 Austria            325 Italy        765\n##  7 Austro-Sardinian            300 Austria            332 Modena       575\n##  8 First Schleswig-Holstein    255 Prussia            390 Denmark      356\n##  9 First Schleswig-Holstein    255 Prussia            390 Denmark      356\n## 10 First Schleswig-Holstein    255 Prussia            390 Denmark      356\n## # ... with 803 more rows\nThe resulting data can be found here.\n\n\n\nWickham H, Hester J and Francois R (2017). readr: Read Rectangular Text Data. R package version 1.1.1, &lt;URL: https://CRAN.R-project.org/package=readr&gt;.\nWickham H, Francois R, Henry L and Müller K (2017). dplyr: A Grammar of Data Manipulation. R package version 0.7.4, &lt;URL: https://CRAN.R-project.org/package=dplyr&gt;.\nWickham H and Henry L (2017). tidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions. R package version 0.7.2, &lt;URL: https://CRAN.R-project.org/package=tidyr&gt;.\nKahle D and Wickham H (2013). “ggmap: Spatial Visualization with ggplot2.” The R Journal, 5(1), pp. 144-161. &lt;URL: http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf&gt;.\nHijmans RJ (2016). geosphere: Spherical Trigonometry. R package version 1.5-5, &lt;URL: https://CRAN.R-project.org/package=geosphere&gt;.\nCheng J, Karambelkar B and Xie Y (2017). leaflet: Create Interactive Web Maps with the JavaScript ‘Leaflet’ Library. R package version 1.1.0, &lt;URL: https://CRAN.R-project.org/package=leaflet&gt;.\nXie Y (2017). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.18, &lt;URL: https://yihui.name/knitr/&gt;.\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, &lt;URL: https://yihui.name/knitr/&gt;.\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F and Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, &lt;URL: http://www.crcpress.com/product/isbn/9781466561595&gt;.\nFrancois R (2017). bibtex: Bibtex Parser. R package version 0.4.2, &lt;URL: https://CRAN.R-project.org/package=bibtex&gt;.\nBoettiger C (2017). knitcitations: Citations for ‘Knitr’ Markdown Files. R package version 1.0.8, &lt;URL: https://CRAN.R-project.org/package=knitcitations&gt;.\n\n\n\nGartzke, Erik, and Alex Braithwaite. 2011. “Power, Parity and Proximity.”\nGleditsch, Kristian S, and Michael D Ward. 2001. “Measuring space: A minimum-distance database and applications to international studies.” Journal of Peace Research 38 (6)."
  },
  {
    "objectID": "content/notes/dists.html#calculating-historical-intercapital-distances",
    "href": "content/notes/dists.html#calculating-historical-intercapital-distances",
    "title": "Brendan Cooley",
    "section": "",
    "text": "Last updated 10 January 2018\nMilitary power degrades with distance. Fighting a war on another continent is, for most militaries, more difficult than fighting at home. Studies of conflict frequently employ capital-to-capital distance (or some transformation of this metric) as one proxy for this loss of strength from power projection (see, for example, (Gartzke and Braithwaite (2011)). Combined with data on territorial contiguity, these metrics can provide us with a picture of the geographic constraints facing countries contemplating war with one another.\nHistorical intercapital distance data proved difficult to find however. Most researchers appear to rely on EuGene to generate this data. I wasn’t able to track down any documentation on exactly how EuGene does this, however.1 Gleditsch and Ward (2001) generated a minimum interstate distance dataset, but their data only covers the post-1875 period. For researchers using Correlates of War data, distance data would ideally cover 1816 to the present.\nIt turns out that it’s not too difficult to build intercapital distance data from scratch, however. All that is required is data on the names of capital cities for each state system member for every year between 1816 and the present. Paul Hensel’s ICOW Historical State Names dataset provides this information. We can then use Google’s Geocoding API through the ggmap R package to get the coordinates of each historical capital, which can then be used to generate intercapital distance matrices.\nHere, I show how to conduct this exercise in R. A clean version of Hensel’s historical capital dataset is available here. I’ve included all of the data and software necessary to generate this data on Github. Feel free to send along questions, comments, or suggestions for improvement to bcooley@princeton.edu.\n\n\nStart by loading up the packages we’ll need for analysis:\nlibs &lt;- c('readr', 'dplyr', 'tidyr', 'ggmap', 'geosphere', 'leaflet', 'knitr', 'bibtex', 'knitcitations')\nsapply(libs, require, character.only = TRUE)\nI started by cleaning up Hensel’s data a bit in excel. Each capital city is listed as its own observation, along with the country, its COW code, and the first and last year the city served as a capital. Because the Hensel data is coded annually, I take the country’s capital at the start of any given year to be its capital for that entire year. For countries that no longer exist (e.g. Mecklenburg-Schwerin) I provide a contemporary alternative country name (aName) to help Google locate the city’s coordinates. The assumption underlying this procedure is that the cities that served as capitals historically have not moved from their historical location (if I’m missing any instances where this occured please let me know). We can take a look at the data below:\n# load data\ndists &lt;- read_csv('capitals.csv')\n\n# Hensel's data run from 1800-2016, set bounds\ndists$startDate &lt;- ifelse(is.na(dists$startDate), 1800, dists$startDate)\ndists$endDate &lt;- ifelse(is.na(dists$endDate), 2016, dists$endDate)\n\nhead(dists)\n## # A tibble: 6 x 6\n##   ccode Name                     aName startDate endDate Capital\n##   &lt;int&gt; &lt;chr&gt;                    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n## 1     2 United States of America &lt;NA&gt;       1800    2016 Washington D.C.\n## 2    20 Canada                   &lt;NA&gt;       1800    1841 Ottawa\n## 3    20 Canada                   &lt;NA&gt;       1841    1843 Kingston\n## 4    20 Canada                   &lt;NA&gt;       1843    1849 Montreal\n## 5    20 Canada                   &lt;NA&gt;       1849    1859 Toronto\n## 6    20 Canada                   &lt;NA&gt;       1859    1865 Quebec City\nNow we need to get the cities’ coordinates. I simply feed the City, Country tuples to ggmap’s geocode function, which returns lat, lng coordinates if it can find a match in Google’s database. If the country has a contemporary name, I use this name for the search in lieu of its old name. I save these coordinates to the dists data frame when they are found. The geocode API imposes some query limits, so it sometimes throws an OVER_QUERY_LIMIT error. If cities remain uncoded, I simply run the loop again, skipping over those that already have coordinates. We can check that the coding worked with the dists %&gt;% filter(is.na(lat)), which should return an empty data frame if all cities have been geocoded.\n# initialize coordinates\ndists$lat &lt;- NA\ndists$lng &lt;- NA\n\n# run until all cities have been coded\nwhile(nrow(dists %&gt;% filter(is.na(lat))) &gt; 0) {\n  # for each city\n  for (i in 1:nrow(dists)) {\n    # skip if it's already been coded\n    if (is.na(dists[i,]$lat)) {\n      if (is.na(dists[i,]$aName)) {\n        # query City, Country for each capital\n        query &lt;- paste0(dists[i, ]$Capital, \", \", dists[i, ]$Name)\n        latlng &lt;- geocode(query)\n        dists[i, ]$lat &lt;- latlng$lat\n        dists[i, ]$lng &lt;- latlng$lon\n      }\n      else {\n        # use alternative country name\n        query &lt;- paste0(dists[i, ]$Capital, \", \", dists[i, ]$aName)\n        latlng &lt;- geocode(query)\n        dists[i, ]$lat &lt;- latlng$lat\n        dists[i, ]$lng &lt;- latlng$lon\n      }\n    }\n  }\n}\nWe can check to make sure the cities were coded correctly by plotting them on a map. Clicking on the city will show a popup with its name. I poked around this map a bit and everything seemed to land in the right spot.\nleaflet(data=dists) %&gt;% addProviderTiles(providers$CartoDB.Positron, options=providerTileOptions(minZoom = 1)) %&gt;%\n  addCircleMarkers(~lng, ~lat, popup=~Capital, radius=1) %&gt;%\n  setMaxBounds(-180, -90, 180, 90)\n\n\nI then export the geocoded capitals as a csv. You can find these data here.\nwrite_csv(dists, 'dists.csv')\n\n\n\nRemember that the point of all this was to generate intercapital distance data for all countries in the COW data. To do this, we want to convert our geocoded capital city data into a dyadic time series that gives the distance between any two countries’ capitals for a given year. If there are \\(N\\) countries in the system in a given year \\(t\\), we want to be able to generate an \\(N \\times N\\) matrix for that year where each entry is the distance between countries \\(i\\) and \\(j\\).\nI start by loading up COW’s state system membership data, so we know which countries were members of the system for each year. The field “styear” denotes the year the country entered the system, and the field “endyear” gives the year it exited. I then append this data to the distance data. The output is shown below.\ndists &lt;- read_csv('dists.csv')\n\n# get state system membership (COW)\nsysMemUrl &lt;- \"http://www.correlatesofwar.org/data-sets/state-system-membership/states2016/at_download/file\"\n\nsysMem &lt;- read_csv(sysMemUrl) %&gt;% select(ccode, styear, endyear)\ndists &lt;- left_join(dists, sysMem, by=\"ccode\")\n\nhead(dists)\n## # A tibble: 6 x 10\n##   ccode Name         aName startD… endDa… Capital    lat   lng stye… endy…\n##   &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt;  &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n## 1     2 United Stat… &lt;NA&gt;     1800   2016 Washing…  38.9 -77.0  1816  2016\n## 2    20 Canada       &lt;NA&gt;     1800   1841 Ottawa    45.4 -75.7  1920  2016\n## 3    20 Canada       &lt;NA&gt;     1841   1843 Kingston  44.2 -76.5  1920  2016\n## 4    20 Canada       &lt;NA&gt;     1843   1849 Montreal  45.5 -73.6  1920  2016\n## 5    20 Canada       &lt;NA&gt;     1849   1859 Toronto   43.7 -79.4  1920  2016\n## 6    20 Canada       &lt;NA&gt;     1859   1865 Quebec …  46.8 -71.2  1920  2016\nFrom this dataframe, I can build the \\(N \\times N\\) intercapital distance matrix for any year. I wrap this in a function coord2DistM, which takes the desired year and the dists dataframe as arguments. It filters the distance data to include only states that were active in that year. It then feeds the latitude and longitude coordinates to the distm function from the geosphere package, which retuns the desired distance matrix. I convert all distances to kilometers.\nThe function distM2dydist takes this distance matrix and a pair of COW country codes and returns the dyadic distance. Below, I show how to use these functions to build the intercapital distance matrix for the year 1816 and get the distance between Britain and Saxony.\n# Note: assigns capital to city that was capital at beginning of year\n\ncoord2DistM &lt;- function(dists, year) {\n  # filter by system membership, then relevant capital\n  distsY &lt;- dists %&gt;% filter(styear &lt;= year, endyear &gt;= year) %&gt;% filter(startDate &lt; year & endDate &gt;= year)\n  # check that one capital returned per country\n  if (length(unique(distsY$Capital)) != length(unique(distsY$Name))) {\n    print('error: nCountries != nCapitals, check underlying coordinate data')\n  }\n  else {\n    # get distance matrix for selected year\n    latlng &lt;- distsY %&gt;% select('lng', 'lat') %&gt;% as.matrix()\n    distsYmatrix &lt;- distm(latlng, latlng, fun=distVincentySphere)\n    distsYmatrix &lt;- distsYmatrix / 1000  # convert to km\n    rownames(distsYmatrix) &lt;- colnames(distsYmatrix) &lt;- distsY$ccode\n    return(distsYmatrix)\n  }\n}\n\n# get distance between i and j\ndistM2dydist &lt;- function(distM, ccode1, ccode2) {\n  return(distM[ccode1, ccode2])\n}\n\n# application\nyear &lt;- 1816\nBritain &lt;- \"200\"\nSaxony &lt;- \"269\"\n\ndistM1816 &lt;- coord2DistM(dists, year)\ndistM2dydist(distM1816, Britain, Saxony)\n## [1] 965.3628\nThese functions can be used in tandem to grab intercapital distances for arbitrary year, dyad pairings.\n\n\n\nConflict researchers often want this data to analyze wars. Now I show how to merge intercapital distance data with COW’s inter-state-war data. I clean up the COW war data a bit, which you can see below post-cleaning. In the most basic leve, the data give information about the belligerents in every war and how long each war lasted.\n# append to COW wars data\nwarUrl &lt;- \"http://www.correlatesofwar.org/data-sets/COW-war/inter-state-war-data/at_download/file\"\ncowWars &lt;- read_csv(warUrl)\n\n# for simplicity, ignore armistices\ncowWars$StartYear &lt;- cowWars$StartYear1\ncowWars$EndYear &lt;- ifelse(cowWars$EndYear2 == -8, cowWars$EndYear1, cowWars$EndYear2)\ncowWars &lt;- cowWars %&gt;% select(WarName, ccode, StateName, Side, StartYear, EndYear)\n\nhead(cowWars)\n## # A tibble: 6 x 6\n##   WarName             ccode StateName                 Side StartYear EndY…\n##   &lt;chr&gt;               &lt;int&gt; &lt;chr&gt;                    &lt;int&gt;     &lt;int&gt; &lt;int&gt;\n## 1 Franco-Spanish War    230 Spain                        2      1823  1823\n## 2 Franco-Spanish War    220 France                       1      1823  1823\n## 3 First Russo-Turkish   640 Ottoman Empire               2      1828  1829\n## 4 First Russo-Turkish   365 Russia                       1      1828  1829\n## 5 Mexican-American       70 Mexico                       2      1846  1847\n## 6 Mexican-American        2 United States of America     1      1846  1847\nWe want to know the intercapital distance between each pair of belligerents between 1816 and the present. We could use the coord2DistM and distM2dydist but this would require calculating the intercapital distance matrix for every year in which there was a war. A simpler solution is to build a dataframe of capital-years, along with their coordinates, merge this with the war data, and calculate the distance between belligerents, given their capitals’ coordinates.\nI used the procedure in this stackoverflow post to create the capital-year dataframe. Once merged, I use geosphere’s distVicentySphere function to calculate the distances. The resulting data is shown below the code.\n# convert capital data to yearly observations\n# https://stackoverflow.com/questions/28553762/expand-year-range-in-r\ndistsYear &lt;- dists\ndistsYear$year &lt;- mapply(seq, distsYear$startDate, distsYear$endDate, SIMPLIFY=FALSE)\ndistsYear &lt;- distsYear %&gt;%\n  unnest(year) %&gt;%\n  select(ccode, year, lat, lng)\n\n# get capital in start year\ncowWars$year &lt;- cowWars$StartYear\n\n# append coords for each side\ncowWars1 &lt;- cowWars %&gt;% filter(Side == 1) %&gt;% left_join(distsYear, by=c(\"ccode\", \"year\")) %&gt;% rename(State1 = StateName, ccode1 = ccode, lat1 = lat, lng1 = lng) %&gt;% select(-Side)\ncowWars2 &lt;- cowWars %&gt;% filter(Side == 2) %&gt;% left_join(distsYear, by=c(\"ccode\", \"year\")) %&gt;% rename(State2 = StateName, ccode2 = ccode, lat2 = lat, lng2 = lng) %&gt;% select(WarName, State2, ccode2, lat2, lng2)\n\ncowWarsDyadic &lt;- left_join(cowWars1, cowWars2, by=\"WarName\") %&gt;% select(WarName, ccode1, State1, ccode2, State2, year, lng1, lat1, lng2, lat2)\n\n# calculate distance\nlatlng1 &lt;- cowWarsDyadic %&gt;% select(lng1, lat1)\nlatlng2 &lt;- cowWarsDyadic %&gt;% select(lng2, lat2)\ncowWarsDyadic$distance &lt;- distVincentySphere(latlng1, latlng2) / 1000  # convert to km\n\n# export data\nwrite_csv(cowWarsDyadic, 'cowWarsDist.csv')\n\ncowWarsDyadic %&gt;% select(WarName, ccode1, State1, ccode2, State2, distance)\n## # A tibble: 813 x 6\n##    WarName                  ccode1 State1          ccode2 State2    dista…\n##    &lt;chr&gt;                     &lt;int&gt; &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n##  1 Franco-Spanish War          220 France             230 Spain       1054\n##  2 Franco-Spanish War          220 France             230 Spain       1054\n##  3 First Russo-Turkish         365 Russia             640 Ottoman …   2233\n##  4 Mexican-American              2 United States …     70 Mexico      3035\n##  5 Austro-Sardinian            300 Austria            337 Tuscany      632\n##  6 Austro-Sardinian            300 Austria            325 Italy        765\n##  7 Austro-Sardinian            300 Austria            332 Modena       575\n##  8 First Schleswig-Holstein    255 Prussia            390 Denmark      356\n##  9 First Schleswig-Holstein    255 Prussia            390 Denmark      356\n## 10 First Schleswig-Holstein    255 Prussia            390 Denmark      356\n## # ... with 803 more rows\nThe resulting data can be found here.\n\n\n\nWickham H, Hester J and Francois R (2017). readr: Read Rectangular Text Data. R package version 1.1.1, &lt;URL: https://CRAN.R-project.org/package=readr&gt;.\nWickham H, Francois R, Henry L and Müller K (2017). dplyr: A Grammar of Data Manipulation. R package version 0.7.4, &lt;URL: https://CRAN.R-project.org/package=dplyr&gt;.\nWickham H and Henry L (2017). tidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions. R package version 0.7.2, &lt;URL: https://CRAN.R-project.org/package=tidyr&gt;.\nKahle D and Wickham H (2013). “ggmap: Spatial Visualization with ggplot2.” The R Journal, 5(1), pp. 144-161. &lt;URL: http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf&gt;.\nHijmans RJ (2016). geosphere: Spherical Trigonometry. R package version 1.5-5, &lt;URL: https://CRAN.R-project.org/package=geosphere&gt;.\nCheng J, Karambelkar B and Xie Y (2017). leaflet: Create Interactive Web Maps with the JavaScript ‘Leaflet’ Library. R package version 1.1.0, &lt;URL: https://CRAN.R-project.org/package=leaflet&gt;.\nXie Y (2017). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.18, &lt;URL: https://yihui.name/knitr/&gt;.\nXie Y (2015). Dynamic Documents with R and knitr, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, &lt;URL: https://yihui.name/knitr/&gt;.\nXie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden V, Leisch F and Peng RD (eds.), Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595, &lt;URL: http://www.crcpress.com/product/isbn/9781466561595&gt;.\nFrancois R (2017). bibtex: Bibtex Parser. R package version 0.4.2, &lt;URL: https://CRAN.R-project.org/package=bibtex&gt;.\nBoettiger C (2017). knitcitations: Citations for ‘Knitr’ Markdown Files. R package version 1.0.8, &lt;URL: https://CRAN.R-project.org/package=knitcitations&gt;.\n\n\n\nGartzke, Erik, and Alex Braithwaite. 2011. “Power, Parity and Proximity.”\nGleditsch, Kristian S, and Michael D Ward. 2001. “Measuring space: A minimum-distance database and applications to international studies.” Journal of Peace Research 38 (6)."
  },
  {
    "objectID": "content/notes/dists.html#footnotes",
    "href": "content/notes/dists.html#footnotes",
    "title": "Brendan Cooley",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTips on where to find this documentation are more than welcome.↩︎"
  }
]