---
title: "The Efron and Morris (1975) Data and the Beta-Binomial Model"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
jupyter: notes
---

You're a baseball fan in April 1970. Player's batting averages for the current season are published in tables at the back of the Sunday sports section. You'd like to use these data to predict how players will perform for the rest of the season. 

Efron and Morris started by collecting batting averages for a sample of players who each had 45 at bats. We will refer to these data as the "training data". They returned to the Sunday sports section to record these players' batting averages after the 162 game season had been completed. We can load these data into memory using `pandas`:

```{python, code_fold="true"}
import pandas as pd

DATA_URL = "https://d2hg8soec8ck9v.cloudfront.net/datasets/EfronMorrisBB.txt"
df = pd.read_csv(DATA_URL, sep="\t")
df["PlayerName"] = df["FirstName"] + " " + df["LastName"]
df_train = df[["PlayerName", "At-Bats", "Hits"]]
df_test = df[["PlayerName", "SeasonAt-Bats", "SeasonHits"]]
```

To start, we'll peak at the training data to get a sense of what we're working with. We can sort the data by the number of hits to see who the best hitters were at the beginning of the season:
```{python}
df_train.sort_values("Hits", ascending=False)
```

A simple way to predict players' batting average for the remainder of the season is simply to use their averages from the first 45 at bats. We can compute these by dividing the number of hits by the number of at bats:

```{python}
df_train["avg"] = df_train["Hits"] / df_train["At-Bats"]
df_train.sort_values("avg", ascending=False)
```

The Bayesian approach asks us 
1. What is the *data generating process*? How would you go about simulating hits and at bats using the tools of probability?
2. What are our prior beliefs. What *should* the distribution of player season-long batting averages look like? 

The beta distribution is defined over probabilities ($[0,1]$). We can toggle the parameters of the distribution to encode our prior beliefs:

```{python}
# from ipywidgets import interact
# from ipywidgets.embed import embed_minimal_html
# from scipy import stats
# import numpy as np
# import altair as alt

# def beta_pdf(alpha, beta):
#   x = np.linspace(0, 1, 100)
#   y = stats.beta.pdf(x, alpha, beta)
#   return alt.Chart(pd.DataFrame({"x": x, "y": y})).mark_line().encode(x="x", y="y")

# beta_pdf(1, 1)
```

```{python, code_fold="true"}
import bokeh
from bokeh.io import output_notebook
import distribution_explorer as de

output_notebook()
```

```{python, code_fold="true"}
bokeh.plotting.show(
  de.explore(
    "beta", 
    params=[
      {"name": "alpha", "start": 1, "end": 500, "value": 25, "step": 5, "min_value": 1, "max_value": 1000}, 
      {"name": "beta", "start": 1, "end": 500, "value": 75, "step": 5, "min_value": 1, "max_value": 1000}
    ],
    x_min=0, 
    x_max=.5,
  )
)
```

We can then model hits as being drawn from a Binomial distribution with probability of success given by the beta distribution. For the first 45 at bats our prior predictive distribution looks something like this:

```{python}
bokeh.plotting.show(
  de.explore(
    "binomial", 
    params=[
      {"name": "N", "start": 1, "end": 50, "value": 50, "step": 1, "min_value": 1, "max_value": 1000}, 
      {"name": "theta", "start": 0, "end": 1, "value": .25, "step": .1, "min_value": 0, "max_value": 1}
    ],
    x_min=0, 
    x_max=50,
  )
)
```

For many simulated seasons of at bats, we expect the distribution of hits for a true .250 batter to look like this
```{python}
bokeh.plotting.show(
  de.explore(
    "binomial", 
    params=[
      {"name": "N", "start": 1, "end": 600, "value": 600, "step": 5, "min_value": 1, "max_value": 1000}, 
      {"name": "theta", "start": 0, "end": 1, "value": .25, "step": .01, "min_value": 0, "max_value": 1}
    ],
    x_min=50, 
    x_max=350,
  )
)
```

With our prior belief, set, we can compute our posterior for our Beta-Binomial model using Bayes' rule. The posterior for this model turns out to be
$$
\text{BA} = \frac{\text{Hits} + \alpha}{\text{At Bats} + \alpha + \beta}
$$

For $\alpha=25$, $\beta=75$ (prior mean of .25), we can compute the posterior for each player in the training data:

```{python}
ALPHA = 25
BETA = 75

df_train["bayes_avg"] = (df_train["Hits"] + ALPHA) / (df_train["At-Bats"] + BETA + ALPHA)
```

And compare our Bayesian predictions to the simple batting averages:

```{python}
import altair as alt

df_long = df_train[['PlayerName', 'avg', 'bayes_avg']].melt(id_vars="PlayerName", var_name="model", value_name="batting_average")

alt.Chart(df_long).mark_line(point=True).encode(
    y='model',
    x=alt.X("batting_average").scale(zero=False),
    tooltip=['PlayerName'],
    color="PlayerName"
).encode().properties(height=150, width=750)
```

Which approach performs better at predicting rest of season averages?

```{python}

results = df_test.merge(df_train[["PlayerName", "avg", "bayes_avg"]], on="PlayerName")
results["season_avg"] = results["SeasonHits"] / results["SeasonAt-Bats"]

vline = alt.Chart(
  pd.DataFrame({"x": [.1, .45], "y": [.1, .45]})
).mark_line(color='red').encode(
  x="x",
  y="y"
)
alt.Chart(results).mark_point(fill='black', color='black').encode(
  x=alt.X("avg").scale(zero=False, domain=[.1, .45]),
  y=alt.Y("season_avg").scale(zero=False, domain=[.1, .45]),
) + vline

```

```{python}
alt.Chart(results).mark_point(fill='black', color='black').encode(
  x=alt.X("bayes_avg").scale(zero=False, domain=[.1, .45]),
  y=alt.Y("season_avg").scale(zero=False, domain=[.1, .45]),
) + vline
```

Error
```{python}
from sklearn.metrics import mean_squared_error

simple_loss = mean_squared_error(results["season_avg"], results["avg"])
bayes_loss = mean_squared_error(results["season_avg"], results["bayes_avg"])
simple_loss, bayes_loss
```

Efron and Morris

![](img/efron_morris.jpeg)


## References 

[1] Efron B., Morris C. (1975), "Data analysis using Stein's estimator and its
    generalizations", J. Amer. Statist. Assoc., 70, 311-319.
[2] `pyro`, [Deep universal probabilistic programming with Python and PyTorch](https://github.com/pyro-ppl/pyro/blob/dev/examples/baseball.py)
[3] Albert, Jim. [Interview with Carl Morris](https://chance.amstat.org/2014/09/carl-morris/).